services:
  qdrant:
    container_name: crawler-qdrant
    image: qdrant/qdrant:v1.15.1
    ports:
      - "${QDRANT_HTTP_PORT:-7000}:6333"
      - "${QDRANT_GRPC_PORT:-7001}:6334"  # gRPC port
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__LOG_LEVEL: "${QDRANT_LOG_LEVEL:-INFO}"
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    healthcheck:
      test: ["CMD", "bash", "-c", "echo -e 'GET /readyz HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' | bash -c 'exec 3<>/dev/tcp/localhost/6333; cat >&3; head -1 <&3' | grep -q '200 OK'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - crawler-network

  text-embeddings-inference:
    image: ghcr.io/huggingface/text-embeddings-inference:1.8
    container_name: crawler-embeddings
    ports:
      - "${TEI_HTTP_PORT:-8080}:80"
    volumes:
      - tei_data:/data
    command:
      - --model-id
      - Qwen/Qwen3-Embedding-0.6B
      - --dtype
      - float16
      - --max-concurrent-requests
      - "80"
      - --max-batch-tokens
      - "163840"
      - --max-batch-requests
      - "80"
      - --max-client-batch-size
      - "128"
      - --pooling
      - "last-token"
      - --tokenization-workers
      - "8"
      - --auto-truncate
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
      - OMP_NUM_THREADS=8
      - TOKENIZERS_PARALLELISM=true
      - CUDA_CACHE_DISABLE=0
      - RUST_LOG=text_embeddings_router=info
      - HF_HUB_CACHE=/data/cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - crawler-network

  # Combined MCP and Webhook Server
  crawler-mcp:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: crawler-mcp-server
    ports:
      - "${SERVER_PORT:-8010}:8010"      # MCP server
      - "${WEBHOOK_PORT:-38080}:38080"   # Webhook server
    volumes:
      - ./data:/app
    env_file: .env
    environment:
      # Service-to-service URLs (must be here for Docker networking)
      - QDRANT_URL=http://qdrant:6333
      - TEI_URL=http://text-embeddings-inference:80
      # GPU-related environment variables
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
      - CUDA_CACHE_DISABLE=0

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    depends_on:
      - qdrant
      - text-embeddings-inference

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health", "&&", "curl", "-f", "http://localhost:38080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 45s

    restart: unless-stopped
    networks:
      - crawler-network

    labels:
      - "com.docker.compose.project=crawl-mcp"
      - "com.docker.compose.service=crawler-mcp"

volumes:
  tei_data:
  qdrant_data:

networks:
  crawler-network:
    driver: bridge
